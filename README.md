# zsc-llm â€” Notebookâ€‘accurate Hierarchical Zeroâ€‘Shot Classification

> Build a **label tree** from your label names/descriptions and classify texts **levelâ€‘byâ€‘level** with fast embedding similarity. Clean Python API + simple CLI. Works with **TFâ€‘IDF** (default) or **Sentenceâ€‘Transformers**.

---

## âœ¨ Features

- **Notebookâ€‘accurate**: mirrors the methodology you prototyped (build hierarchy â†’ route per level)
- **Plug & play**: start with just **label names** (descriptions optional, but recommended)
- **Interpretable**: returns the **full decision path** through the tree to the leaf class
- **Flexible embeddings**: TFâ€‘IDF (CPU, fast) or Sentenceâ€‘Transformers (GPUâ€‘ready)
- **Readable nodes**: internal clusters named via **keywords** or **child labels**
- **Scoring**: choose **logâ€‘prob** (additive, stable) or **prob** (0..1)
- **CLI & Python**: one command or a few lines of code

---

## ðŸ“¦ Installation

```bash
# clone your repo (or unzip the folder)
git clone <your-repo-url>.git
cd zsc-llm

# install in editable mode into the current Python environment
pip install -e .
```

> **Colab/Jupyter tip**: run `pip install -e .` **inside a notebook cell** so it installs into the same kernel.
>
> If you choose not to install, you can temporarily add the path:
> ```python
> import sys; sys.path.append("/content/zsc-llm/src")
> ```

### Requirements
- Python â‰¥ 3.9
- `numpy`, `scikit-learn`
- Optional: `sentence-transformers` (for transformer embeddings & GPU)

---

## ðŸ§  How it works (one minute)

1. Provide **labels** (and optionally **descriptions**).
2. We embed label descriptions (or label names if descriptions arenâ€™t provided).
3. We **cluster labels** recursively (divisive kâ€‘means) â†’ build a tree: internal **nodes** â†’ **leaves** (labels).
4. For each internal node we compute **prototypes** for children and install a **router** (cosine sim â†’ softmax).
5. At inference:
   - Embed text, start at **Root**, choose best child, **descend** levelâ€‘byâ€‘level.
   - Optional **beam search** explores multiple branches per level.
   - Return **topâ€‘k paths** and the **leaf** as the predicted class.

---

## ðŸš€ Quick start â€” CLI

### A) Using **label descriptions** (recommended)
`label_descs.json` maps label â†’ richer description.

```bash
zsc-llm predict-notebook \
  --label-descs-file samples/label_descs_dense.json \
  --input samples/stress_texts.txt \
  --embedder tfidf \
  --beam 5 --topk-paths 5 \
  --scores prob \
  --node-names keywords \
  --out paths.csv
```

- `--label-descs-file`: JSON like `{ "LabelName": "longer description", ... }`
- `--scores`: `prob` (0..1) or `log` (sum of logâ€‘probs along the path)
- `--node-names`: `keywords` (TFâ€‘IDF topic), `children` (child label tokens), or `none`
- Use Sentenceâ€‘Transformers:
  ```bash
  zsc-llm predict-notebook \
    --label-descs-file samples/label_descs_dense.json \
    --input samples/stress_texts.txt \
    --embedder st --st-model sentence-transformers/all-MiniLM-L6-v2 --device cuda:0 \
    --beam 5 --topk-paths 5 --scores prob
  ```

### B) Using **labels only** (descriptions optional)
TXT file with one label per line, or a JSON list.

```bash
zsc-llm predict-notebook \
  --labels-file samples/labels_min.txt \
  --input samples/long_example.txt \
  --embedder tfidf \
  --beam 3 --topk-paths 3 \
  --scores prob
```

### Input formats
- `.txt` â€” one text per line
- `.jsonl` â€” one JSON object per line; use `--text-key` to specify the field (default: `text`)
- `.csv` â€” supply `--text-col <column name>`

**Output**: a CSV with columns  
`index, text, path, path_prob|log_score, top_leaf`

---

## ðŸ Quick start â€” Python

### A) With **descriptions**
```python
from zsc_llm.nb_method import ZeroShotHierarchicalClassifier, SimpleTFIDFEmbedder
import json, pathlib

label_texts = json.loads(pathlib.Path("samples/label_descs_dense.json").read_text())

clf = ZeroShotHierarchicalClassifier(
    branching_factor=8,
    min_cluster_size=1,
    embedder=SimpleTFIDFEmbedder(),  # TF-IDF (fast, CPU)
    sim_temperature=0.7,
    internal_naming="keywords",      # or "children" / "none"
).fit(label_texts=label_texts)

text = "ACH to my external bank is pending for days and I also see a foreign transaction fee on a hotel booking."
paths = clf.predict_paths([text], topk_paths=5, beam=5, score_mode="prob")

# Full paths with probabilities
for path, prob in paths[0]:
    print(" > ".join(path), f"| path_prob={prob:.4f}")

# Best class (leaf)
best_path, best_prob = paths[0][0]
best_class = best_path[-1]
print(f"\nBest class: {best_class}  (path_prob={best_prob:.4f})")
```

### B) With **labels only**
```python
from zsc_llm.nb_method import ZeroShotHierarchicalClassifier, SimpleTFIDFEmbedder

labels = ["Billing","Refunds","TechSupport","AppBugs","CardIssues","AccountClosure"]

clf = ZeroShotHierarchicalClassifier(
    embedder=SimpleTFIDFEmbedder(),
    internal_naming="children"  # robust naming even without descriptions
).fit(labels=labels)

print(clf.predict_paths(["App crashed during transfer and card declined at POS."],
                        topk_paths=3, beam=3, score_mode="prob")[0])
```

### C) With **Sentenceâ€‘Transformers** (GPU)
```python
from zsc_llm.nb_method import ZeroShotHierarchicalClassifier, SentenceTransformerEmbedder

clf = ZeroShotHierarchicalClassifier(
    embedder=SentenceTransformerEmbedder("sentence-transformers/all-MiniLM-L6-v2", device="cuda:0"),
    internal_naming="children"
).fit(labels=["Billing","Refunds","TechSupport","AppBugs","CardIssues","AccountClosure"])
```

---

## ðŸ” Output explained

A path looks like:
```
Root > node[ach pending wire] > node[foreign fee fx] > Fee_ForeignTxn | path_prob=0.42
```
- **Internal nodes** are clusters; named from **TFâ€‘IDF keywords** or **child labels** so you can read them.
- **Leaf** (last segment) is the predicted class.
- `path_prob` multiplies perâ€‘level probabilities (converted from logâ€‘probs for readability).
- Prefer `score_mode="log"` for numerically stable **additive** scores during analysis.

---

## âš™ï¸ Key parameters

- `branching_factor` â€” max children per split (default **8**)
- `min_cluster_size` â€” small clusters merge/fallback (default **1**)
- `beam` â€” keep topâ€‘K branches per level at inference (default **1**)
- `topk_paths` â€” how many full paths to return (default **1**)
- `sim_temperature` â€” softmax temperature at routers (lower â†’ peakier)
- `internal_naming` â€” `keywords` | `children` | `none`
- `score_mode` â€” `prob` | `log`

---

## ðŸ“‚ Samples

- `samples/label_descs_dense.json` â€” **80** closelyâ€‘related classes with overlapping descriptions
- `samples/stress_texts.txt` â€” **50** ambiguous, multiâ€‘issue test lines to stress the hierarchy
- `samples/labels_min.txt` â€” tiny labelsâ€‘only example
- `samples/long_example.txt` â€” long multiâ€‘topic customer message

Run the stress set:
```bash
zsc-llm predict-notebook \
  --label-descs-file samples/label_descs_dense.json \
  --input samples/stress_texts.txt \
  --embedder tfidf \
  --beam 5 --topk-paths 5 --scores prob \
  --node-names keywords \
  --out stress_paths.csv
```

---

## ðŸ§ª Dev & testing

```bash
# run tests
pytest -q
```

Project layout:
```
zsc-llm/
â”œâ”€ src/zsc_llm/
â”‚  â”œâ”€ __init__.py
â”‚  â”œâ”€ nb_method.py          # main implementation (API)
â”‚  â””â”€ cli.py                # CLI: zsc-llm predict-notebook
â”œâ”€ samples/
â”‚  â”œâ”€ label_descs_dense.json
â”‚  â”œâ”€ stress_texts.txt
â”‚  â”œâ”€ labels_min.txt
â”‚  â””â”€ long_example.txt
â”œâ”€ tests/
â”‚  â”œâ”€ test_basic.py
â”‚  â””â”€ test_labels_only.py
â”œâ”€ pyproject.toml
â”œâ”€ README.md
â””â”€ LICENSE
```

---

## ðŸ› ï¸ Troubleshooting

- **I still see `node` in paths**  
  Set `internal_naming="children"` (works even with labelsâ€‘only), reinstall (`pip install -e .`), and reâ€‘run.

- **Import fails in notebook**  
  Ensure you ran `pip install -e .` **in the same kernel**. Otherwise use the temporary path:  
  `import sys; sys.path.append("/content/zsc-llm/src")`

- **Slow ST encoding**  
  Start with TFâ€‘IDF (fast) â†’ switch to Sentenceâ€‘Transformers with `device="cuda:0"` for better semantics.

- **Probabilities look small**  
  Theyâ€™re **path** probabilities (product across levels). Use `score_mode="log"` for additive analysis.

---

## ðŸ“œ License

MIT â€” see `LICENSE`.

---

## ðŸ™ Citation

If you use this code, a simple citation in your README or docs is appreciated:

> *zscâ€‘llm: Notebookâ€‘accurate hierarchical zeroâ€‘shot classification (TFâ€‘IDF/ST embeddings, perâ€‘level routing), 2025.*
